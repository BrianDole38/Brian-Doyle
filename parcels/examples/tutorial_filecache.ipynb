{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Caching fieldsets files in temporary, high-speed storage locations\n",
    "\n",
    "Parcels simulations can be very time-consuming operations. The runtime of each simulation depends on several parameters:\n",
    "\n",
    "* Number of particles\n",
    "* Number of attributes per particle (class)\n",
    "* Lifetime modality of particles: fixed, continuous release, conditional removal, fully dynamic\n",
    "* Number of fields\n",
    "* Grid resolution of fields\n",
    "* Temporal spacing of fields\n",
    "* Computational density of the kernels (i.e. how much code the kernels have)\n",
    "* Computational complexity of the kernels (i.e. how smart and computationally efficient the kernels are written) \n",
    "* Particle file output time period (i.e. _dt_ of writing particles)\n",
    "* Integration period (i.e. computational _dt_ in trajectory interation)\n",
    "* etc.\n",
    "\n",
    "Several of the mentioned parameters can easily be steered (e.g. number of particles), though are strongly dependent of other parameters.\n",
    "Some of the parameters, such as computational complexity of the kernels, are directly controlled by the computational skills of the oceanographer.\n",
    "That said, computation-related parameters have often less impact on the actual simulation runtime.\n",
    "Most of the time, the time expense of a Parcels, Lagrangian particle simulation is controlled by _input/output (I/O)_ operations,\n",
    "which means they are mainly controlled by the demand of fields and their grid properties, which cannot be controlled by the Lagrangian modeller.\n",
    "\n",
    "We have identified and accepted this issue in Lagrangian simulations. As a consequence, the most promising avenue to speed up simulations\n",
    "is to reduce the reading time of field data from files. There are overall 2 options in Parcels to address this issue:\n",
    "\n",
    "1. Chunking field files via _Dask_ to reduce the actual data being read\n",
    "2. Pre-buffering field files in cache locations to speed-up the actual reading of a file.\n",
    "\n",
    "The usage of chunking is demonstrated in the [_example_dask_chunk_OCMs.py_](https://github.com/OceanParcels/parcels/blob/master/parcels/examples/example_dask_chunk_OCMs.py) file. Here, we detail how to use file cache locations to\n",
    "speed up simulations.\n",
    "\n",
    "## Status-quo and problem\n",
    "\n",
    "We assume here to perform a 3D simulation with the NEMO data. We can measure the runtime of this experiment for the simulation execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "WARNING: File '/data/NEMO-MEDUSA/NORTHSEA_ORCA025-N006/coordinates.nc' could not be decoded properly by xarray (version: 0.19.0).\n",
      "         It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib75526eba08dc72fd498cd3a9c2742678_0.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 2.0957419089999996 sec.\n"
     ]
    }
   ],
   "source": [
    "from parcels import FieldSet, ParticleSet, JITParticle, AdvectionRK4_3D\n",
    "from time import process_time as compute_ptimer_hr\n",
    "from datetime import timedelta as delta\n",
    "from glob import glob\n",
    "import gc\n",
    "\n",
    "from parcels.tools import logger\n",
    "# from parcels.tools.loggers import XarrayDecodedFilter\n",
    "# logger.addFilter(XarrayDecodedFilter())  # Add a filter for the xarray decoding warning\n",
    "\n",
    "# data_path = 'NemoNorthSeaORCA025-N006_data/'\n",
    "data_path = \"/data/NEMO-MEDUSA/NORTHSEA_ORCA025-N006/\"\n",
    "ufiles = sorted(glob(data_path+'ORCA*U.nc'))\n",
    "vfiles = sorted(glob(data_path+'ORCA*V.nc'))\n",
    "wfiles = sorted(glob(data_path+'ORCA*W.nc'))\n",
    "mesh_mask = data_path + 'coordinates.nc'\n",
    "\n",
    "filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n",
    "             'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles},\n",
    "             'W': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': wfiles}}\n",
    "\n",
    "variables = {'U': 'uo',\n",
    "             'V': 'vo',\n",
    "             'W': 'wo'}\n",
    "dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n",
    "              'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n",
    "              'W': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n",
    "\n",
    "fieldset = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=False)\n",
    "\n",
    "pset = ParticleSet.from_line(fieldset=fieldset, pclass=JITParticle,\n",
    "                             size=10,\n",
    "                             start=(1.9, 52.5),\n",
    "                             finish=(3.4, 51.6),\n",
    "                             depth=1)\n",
    "\n",
    "kernels = pset.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset.execute(kernels, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "del kernels\n",
    "del pset\n",
    "del fieldset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simply using fieldset buffers - the synchronous option\n",
    "\n",
    "The majority of the runtime here is spent in reading and loading the data from file. We can now simply activate buffering (i.e. _caching_) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib1035c7154af8cab6dc09da1af2864136_0.so\n",
      "WARNING: Removing cache folder '/tmp/christian/parcels-1000/26112' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1.7454694660000003 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2624"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldset_ca_st = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=True)\n",
    "pset_ca_st = ParticleSet.from_line(fieldset=fieldset_ca_st, pclass=JITParticle,\n",
    "                                   size=10,\n",
    "                                   start=(1.9, 52.5),\n",
    "                                   finish=(3.4, 51.6),\n",
    "                                   depth=1)\n",
    "kernels_ca_st = pset_ca_st.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset_ca_st.execute(kernels_ca_st, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "fieldset_ca_st.stop_caching()\n",
    "del kernels_ca_st\n",
    "del pset_ca_st\n",
    "del fieldset_ca_st\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining one's individual buffer location\n",
    "\n",
    "Where did the simulation now store those data ? For some special infrastructures in the Netherlands, Parcels knows ideal caching paths.\n",
    "That said, for you as external user, you would need to define the path of your high-speed cache drive (e.g. _solid-state drive (SSD)_)\n",
    "on fieldset construction. Let us assume your cache drive is located at */data/ssd_drive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib5131fb3f767e120afca7e7879fb14c41_0.so\n",
      "WARNING: Removing cache folder '/tmp/christian/parcels-1000/26112' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1.7731092340000014 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1833"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldset_ca_st_manual = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=True, cache_dir=\"/data/ssd_drive\")\n",
    "pset_ca_st_manual = ParticleSet.from_line(fieldset=fieldset_ca_st_manual, pclass=JITParticle, size=10, start=(1.9, 52.5), finish=(3.4, 51.6), depth=1)\n",
    "kernels_ca_st_manual = pset_ca_st_manual.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset_ca_st_manual.execute(kernels_ca_st_manual, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "fieldset_ca_st_manual.stop_caching()\n",
    "del kernels_ca_st_manual\n",
    "del pset_ca_st_manual\n",
    "del fieldset_ca_st_manual\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Technically, what happens is that with each new timestep Parcels moves the field files of the simulation from the low-throughput\n",
    "storage to the high-throughput buffer location. In a synchronized, single threaded setup as outlined about, the gain of this operation\n",
    "is minimal and only leads to gains with either (a) very large field files with many attributes, or (b) if the throughput difference between\n",
    "low- and high-throughput locations is very big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Speeding up fieldset buffers - the asynchronous option\n",
    "\n",
    "In order to gain more from the buffered process, the buffer management ideally runs in the background. For that, Parcels uses a multi-threaded setup,\n",
    "where a background thread populates the file pool that the simulation thread in the end consumes. We can activate this option with the argument `use_thread=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib0cf1115e84ec405b1433908e91a088d9_0.so\n",
      "WARNING: Removing cache folder '/tmp/christian/parcels-1000/26112' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1.7177553899999989 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1833"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldset_ca_mt = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=True, use_threads=True, cache_dir=\"/data/ssd_drive\")\n",
    "pset_ca_mt = ParticleSet.from_line(fieldset=fieldset_ca_mt, pclass=JITParticle,\n",
    "                                   size=10,\n",
    "                                   start=(1.9, 52.5),\n",
    "                                   finish=(3.4, 51.6),\n",
    "                                   depth=1)\n",
    "kernels_ca_mt = pset_ca_mt.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset_ca_mt.execute(kernels_ca_mt, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "fieldset_ca_mt.stop_caching()\n",
    "del kernels_ca_mt\n",
    "del pset_ca_mt\n",
    "del fieldset_ca_mt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Advanced buffer options\n",
    "\n",
    "The buffer itself is controlled by two variables: a lower- and an upper caching cap. The buffer manager removes old fields (not in use anymore) until the buffer\n",
    "is between upper- and lower cap, and adds new field to the buffers as long as the buffer is below the upper cap. For simulations with many, large fields,\n",
    "those caps can be changed. On clusters and non-personal compute environments, please consult your ICT manager for the max. individual quota for personal\n",
    "scratch locations. To change those caps, the `FieldFileCache` needs to be created by itself, as the upper cap is part of the advanced-level functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib8fadb8fe2e486406b70881d61d2f11f8_0.so\n",
      "WARNING: Removing cache folder '/tmp/christian/parcels-1000/26112' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1.734731759999999 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parcels.tools import FieldFileCache\n",
    "\n",
    "use_threads = False  # or: True (choice)\n",
    "lower_cache_limit = int(0.5*(2**30))\n",
    "upper_cache_limit = int(2.0*(2**30))\n",
    "cache_obj = FieldFileCache(cache_lower_limit=lower_cache_limit, cache_upper_limit=upper_cache_limit, use_thread=use_threads, cache_top_dir=\"/data/ssd_drive\")\n",
    "fieldset_ca_mt_manual = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=True, cache=cache_obj)\n",
    "pset_ca_mt_manual = ParticleSet.from_line(fieldset=fieldset_ca_mt_manual, pclass=JITParticle, size=10, start=(1.9, 52.5), finish=(3.4, 51.6), depth=1)\n",
    "kernels_ca_mt_manual = pset_ca_mt_manual.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset_ca_mt_manual.execute(kernels_ca_mt_manual, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "fieldset_ca_mt_manual.stop_caching()\n",
    "del kernels_ca_mt_manual\n",
    "del pset_ca_mt_manual\n",
    "del cache_obj\n",
    "del fieldset_ca_mt_manual\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are a few expanded options a user can access with this illustrated construct. Aside setting not only the lower- but also the upper cache limit,\n",
    "it is possible to constraint the pre-buffer to a maximum of _m_ future timesteps with the use of the `cache_step_limit` property. This can be useful in\n",
    "long simulations with very small files to avoid a big initial load overhead with the first initial buffer population. Furthermore, using the `debug` parameter\n",
    "in the `FieldFileCache` construction allows to print-out the extensive track of debug information in case a simulation once doesn't work as intended with caching.\n",
    "Accessing the `cache_top_dir` and writing a new path in there allows to circumvent the auto-determination of paths on the Dutch infrastructure. One can also\n",
    "`enable_named_copy` or `disable_named_copy` (default) to control the renaming of files of field variables and split files where _U, V and W_ are written in the same\n",
    "file into multiple files in the cache. This is advisable when combining chunking via _Dask_ and the field buffer together, as it allows for auxilliary variables\n",
    "(e.g. _TPP3_, _CO2_ and others) in the same file to be chunked independently. Lastly, the advanced construction allows to `enable_threading` or `disable_threading` after\n",
    "the `FieldFileCache` construction. Those capabilities are illustrated in the final example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: None: not found\n",
      "INFO: Compiled ArrayJITParticleAdvectionRK4_3D ==> /tmp/parcels-1000/lib9e68b1f5786a189fa1ed51782759bae0_0.so\n",
      "WARNING: Removing cache folder '/tmp/christian/parcels-1000/26112' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1.7673386569999998 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_threads = False\n",
    "lower_cache_limit = int(0.5*(2**30))\n",
    "upper_cache_limit = int(2.0*(2**30))\n",
    "cache_obj_adv = FieldFileCache(cache_lower_limit=lower_cache_limit, cache_upper_limit=upper_cache_limit, use_thread=use_threads, cache_top_dir=\"/data/ssd_drive\")\n",
    "cache_obj_adv.cache_step_limit = 3\n",
    "cache_obj_adv.enable_named_copy()\n",
    "cache_obj_adv.enable_threading()\n",
    "\n",
    "fieldset_ca_mt_adv_manual = FieldSet.from_nemo(filenames, variables, dimensions, time_periodic=delta(days=183).total_seconds(), do_cache=True, cache=cache_obj_adv)\n",
    "pset_ca_mt_adv_manual = ParticleSet.from_line(fieldset=fieldset_ca_mt_adv_manual, pclass=JITParticle, size=10, start=(1.9, 52.5), finish=(3.4, 51.6), depth=1)\n",
    "kernels_ca_mt_adv_manual = pset_ca_mt_adv_manual.Kernel(AdvectionRK4_3D)\n",
    "stime = compute_ptimer_hr()\n",
    "pset_ca_mt_adv_manual.execute(kernels_ca_mt_adv_manual, runtime=delta(days=91), dt=delta(hours=6))\n",
    "etime = compute_ptimer_hr()\n",
    "delta_runtime = etime-stime\n",
    "print(\"Runtime: {} sec.\".format(delta_runtime))\n",
    "fieldset_ca_mt_adv_manual.stop_caching()\n",
    "del kernels_ca_mt_adv_manual\n",
    "del pset_ca_mt_adv_manual\n",
    "del fieldset_ca_mt_adv_manual\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
